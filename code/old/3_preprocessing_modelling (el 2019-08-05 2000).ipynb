{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Preprocessing & Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to use the datasets provided to build a model that predicts the presence of the West Nile Virus. The model is meant for use by the City of Chicago for decisions involving pesticide spray.\n",
    "\n",
    "### Contents:\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Import Data](#Import-Data)\n",
    "- [Data prepared for Modelling](#Data-prepared-for-Modelling)\n",
    "- [Modelling](#Modelling)\n",
    "- [Models Evaluation & Next Steps](#Models-Evaluation-&-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the necessary libraries used in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# maths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visual\n",
    "#from matplotlib_venn import venn2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler,PolynomialFeatures\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix,accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "\n",
    "# Others\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the datasets with standardised file paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "\n",
    "input_path = '../data/2_input/'\n",
    "clean_path = '../data/3_clean/'\n",
    "output_path = '../data/4_output/'\n",
    "\n",
    "image_path = '../images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets imported here have already been cleaned for null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import clean data\n",
    "\n",
    "df_train = pd.read_csv(clean_path + 'train_clean.csv')\n",
    "df_test = pd.read_csv(clean_path + 'test_clean.csv')\n",
    "df_weather = pd.read_csv(clean_path + 'weather_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of train dataset: {}'.format(df_train.shape))\n",
    "print('Size of test dataset: {}'.format(df_test.shape))\n",
    "print('Size of weather dataset: {}'.format(df_weather.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prepared for Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine train and test data to prepare the datasets for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops id column from test not in train\n",
    "test = df_test.drop('id', axis=1)\n",
    "#Drops nummosquitos and wnvpresent columns from train not in test\n",
    "train = df_train.drop(['nummosquitos', 'wnvpresent'], axis=1)\n",
    "\n",
    "#Combines train and test datasets\n",
    "combined_train_test = pd.concat([test,train])\n",
    "\n",
    "print('Size of train/test dataset: {}'.format(combined_train_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather dataset gives us information of weather conditions from 2007 to 2014, during the months of the virus tests. It includes data from two weather stations:\n",
    "\n",
    "<br>Station 1: CHICAGO O'HARE INTERNATIONAL AIRPORT Lat: 41.995 Lon: -87.933 Elev: 662 ft. above sea level\n",
    "<br>Station 2: CHICAGO MIDWAY INTL ARPT Lat: 41.786 Lon: -87.752 Elev: 612 ft. above sea level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data from each station and combine it across columns to prevent duplication of data points on columns in the main dataset. Then merge it with the train/test dataset to add information on weather conditions on the dates of virus test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits weather data by Station\n",
    "only_station_1 = df_weather[df_weather['station'] == 1].reset_index(drop=True)\n",
    "only_station_2 = df_weather[df_weather['station'] == 2].reset_index(drop=True)\n",
    "\n",
    "#Renames Station 2 data for differentiation\n",
    "only_station_2.columns = [str(col) + '_2' for col in only_station_2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine weather data from both stations across columns and drop Station columns\n",
    "parallel_weather = pd.concat([only_station_1,only_station_2], axis=1).drop(['station','station_2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print combined weather data\n",
    "parallel_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines weather data with train and test dataset\n",
    "all_dataset = combined_train_test.merge(parallel_weather, how='left', on=['year','month','day'])\n",
    "\n",
    "print('Size of train/test dataset with weather data: {}'.format(all_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints train/test dataset with weather information\n",
    "all_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical data, we use One Hot Encoding to convert them to numerical data and drop the first column of each categorical feature as it represents duplicated information. We also drop the original columns with non-numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts categorical data into numeric\n",
    "df_get_dum = pd.concat([all_dataset, pd.get_dummies(all_dataset[['species', 'street', 'trap']],drop_first=True)], axis=1)\n",
    "df_get_dum.drop(['species', 'street', 'trap'], inplace =True, axis=1)\n",
    "\n",
    "print('Size of train/test dataset with weather data(One Hot Encoded): {}'.format(df_get_dum.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data back into seperate train and test datasets and only use train for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits out train dataset using year\n",
    "train = df_get_dum[df_get_dum['year']%2!=0]\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#Re-attaching original wnvpresent column\n",
    "wnv = pd.Series(df_train['wnvpresent'])\n",
    "train_with_wnv = pd.concat([train , wnv], axis=1)\n",
    "\n",
    "print('Size of processed train data: {}'.format(train_with_wnv.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits out test dataset using year\n",
    "test = df_get_dum.loc[df_get_dum['year']%2==0]\n",
    "print('Size of processed train data: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also note that the data is imbalanced. Out of the 8475 rows in our training dataset, only 457 (~5%) data points represent the virus present class while 8018 represent virus not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of training data\n",
    "train_with_wnv.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representation of classes\n",
    "train_with_wnv.wnvpresent.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the imbalanced data, we conduct oversampling on the minority class i.e. the data points where wnv is present. While undersampling is an option, we decide that undersampling the majority class to match the size of the minority class dataset will reduce the data points immensely, and is hence not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split the data into the classes. Then, resample minority class with replacement until there are 8018 data points before combining the new minority dataset with the original majority class dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits data by presence of wnv\n",
    "majority_class = train_with_wnv[train_with_wnv['wnvpresent']==0]\n",
    "minority_class = train_with_wnv[train_with_wnv['wnvpresent']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resamples minority class with replacement\n",
    "minority_upsampled = resample( minority_class, replace=True, n_samples=majority_class.shape[0], random_state=42)\n",
    "\n",
    "#Combine new minority class dataset with original majority class dataset\n",
    "train_resampled = pd.concat([minority_upsampled,majority_class])\n",
    "\n",
    "#Checks class representation\n",
    "train_resampled.wnvpresent.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the dataset to inject randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffles dataset\n",
    "df = shuffle(train_resampled, random_state=42)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print resampled, reshuffled new dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use all the features in the dataset to fit classification models and identify wnvpresent to be our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['wnvpresent'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.wnvpresent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into random train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model on the following classifiers and test its performance using ROC/AUC score:\n",
    "    <br> - Logistic Regression, \n",
    "    <br> - KNearestNeighbour, \n",
    "    <br> - Decision Tree,\n",
    "    <br> - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {\n",
    "    'Lr': LogisticRegression(),\n",
    "    'Knn': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Dtree': DecisionTreeClassifier(),\n",
    "    'Rf': RandomForestClassifier()\n",
    "}.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a pipeline to scale the data before fitting to the classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in estimators:\n",
    "    pipe = Pipeline([\n",
    "        ('sc', StandardScaler()),\n",
    "        (k,v)])\n",
    "    model = pipe.fit(X_train,y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    print('{} score: {} Cross-Validation: {}'.format(k, model.score(X_train,y_train), cross_val_score(model,X,y,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Evaluation & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
