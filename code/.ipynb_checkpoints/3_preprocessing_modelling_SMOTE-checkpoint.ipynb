{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Preprocessing & Modelling -SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explore another method of oversampling using the Synthetic Minority Oversampling Technique (SMOTE) which creates new data points using the nearest neighbours of the original data points in the minority class. New data points are created to balance out the classes in the data.\n",
    "\n",
    "### Contents:\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Import Data](#Import-Data)\n",
    "- [Data prepared for Modelling](#Data-prepared-for-Modelling)\n",
    "- [Modelling](#Modelling)\n",
    "- [Models Evaluation & Next Steps](#Models-Evaluation-&-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the necessary libraries used in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# maths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# modelling\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Others\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the datasets with standardised file paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "\n",
    "input_path = '../data/2_input/'\n",
    "clean_path = '../data/3_clean/'\n",
    "output_path = '../data/4_output/'\n",
    "\n",
    "image_path = '../images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets imported here have already been cleaned for null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import clean data\n",
    "\n",
    "df_train = pd.read_csv(clean_path + 'train_clean.csv')\n",
    "df_test = pd.read_csv(clean_path + 'test_clean.csv')\n",
    "df_weather = pd.read_csv(clean_path + 'weather_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of train dataset: {}'.format(df_train.shape))\n",
    "print('Size of test dataset: {}'.format(df_test.shape))\n",
    "print('Size of weather dataset: {}'.format(df_weather.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prepared for Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine train and test data to prepare the datasets for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops id column from test not in train\n",
    "test = df_test.drop('id', axis=1)\n",
    "#Drops nummosquitos and wnvpresent columns from train not in test\n",
    "train = df_train.drop(['nummosquitos', 'wnvpresent'], axis=1)\n",
    "\n",
    "#Combines train and test datasets\n",
    "combined_train_test = pd.concat([test,train])\n",
    "\n",
    "print('Size of train/test dataset: {}'.format(combined_train_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather dataset gives us information of weather conditions from 2007 to 2014, during the months of the virus tests. It includes data from two weather stations:\n",
    "\n",
    "<br>Station 1: CHICAGO O'HARE INTERNATIONAL AIRPORT Lat: 41.995 Lon: -87.933 Elev: 662 ft. above sea level\n",
    "<br>Station 2: CHICAGO MIDWAY INTL ARPT Lat: 41.786 Lon: -87.752 Elev: 612 ft. above sea level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data from each station and use only the data from Station 1 here, considering that there were many null values in Station 2 which we imputed from Station 1 data when cleaning the data. \n",
    "\n",
    "We then, merge the Station 1 weather data with the train/test dataset to add information on weather conditions as measured at Station 1 on the dates of virus test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits weather data by Station\n",
    "only_station_1 = df_weather[df_weather['station'] == 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using weather data only from Station 1\n",
    "all_dataset = combined_train_test.merge(only_station_1, how='left', on=['year','month','day'])\n",
    "print('Size of train/test dataset with Station 1 weather data: {}'.format(all_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print combined data\n",
    "all_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical data, we use One Hot Encoding to convert them to numerical data and drop the first column of each categorical feature as it represents duplicated information. We also drop the original columns with non-numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts categorical data into numeric\n",
    "df_get_dum = pd.concat([all_dataset, pd.get_dummies(all_dataset[['species', 'street', 'trap']],drop_first=True)], axis=1)\n",
    "df_get_dum.drop(['species', 'street', 'trap'], inplace =True, axis=1)\n",
    "\n",
    "print('Size of train/test dataset with weather data(One Hot Encoded): {}'.format(df_get_dum.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data back into seperate train and test datasets and only use train for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits out train dataset using year\n",
    "train = df_get_dum[df_get_dum['year']%2!=0]\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print('Size of processed train data: {}'.format(train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits out test dataset using year\n",
    "test = df_get_dum.loc[df_get_dum['year']%2==0]\n",
    "print('Size of processed test data: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use all the features in the dataset to fit classification models and identify wnvpresent to be our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train.wnvpresent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conduct oversampling on the minority class using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "\n",
    "#Checks class representation\n",
    "pd.Series(y_resampled).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into random train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model on Random Forest Classifer and tune the hyperparameters with GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tuning hyperparameters through GridSearch<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators' : [10, 50, 100],\n",
    "    'max_depth' : [3,9,15,20],\n",
    "    'min_samples_split': np.linspace(0.1, 0.5, 5),\n",
    "    'min_samples_leaf' : np.linspace(0.1, 0.5, 5),\n",
    "    'max_features' : (20, 50, 200, None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = []\n",
    "roc_auc = []\n",
    "\n",
    "gridsearch = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=params,\n",
    "    verbose=1,\n",
    "    cv= 3,\n",
    "    n_jobs=-1,\n",
    "    return_train_score= True,\n",
    "    scoring = 'roc_auc'\n",
    ")\n",
    "\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "model = gridsearch.best_estimator_\n",
    "cv_score = gridsearch.cv_results_\n",
    "best_params = gridsearch.best_params_\n",
    "\n",
    "# predict y\n",
    "y_pred = pd.DataFrame(model.predict_proba(X_test), columns=['0','1'])\n",
    "\n",
    "# print results\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", gridsearch.best_score_)\n",
    "print(\"AUC/ROC test:\", roc_auc_score(y_test,y_pred['1']))\n",
    "pd.set_option('display.max_rows', 750)\n",
    "display(pd.DataFrame(cv_score, columns = cv_score.keys()), )\n",
    "\n",
    "\n",
    "# append info to list\n",
    "parameters.append(best_params)\n",
    "roc_auc.append(roc_auc_score(y_test,y_pred['1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({\n",
    "    'feature':X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "\n",
    "fig = fi.sort_values('importance', ascending=False).iloc[:20]\n",
    "fig.plot(kind='barh', figsize=(10,6))\n",
    "plt.yticks(range(len(fig)),fig['feature'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv for kaggle submission\n",
    "features = list(X.columns)\n",
    "test = test[features]\n",
    "\n",
    "pred = pd.DataFrame(model.predict_proba(test), columns=['0','1']) \n",
    "submission = pd.DataFrame()\n",
    "submission['WnvPresent'] = pred['1']\n",
    "submission['Id'] = submission.index + 1\n",
    "submission[['Id', 'WnvPresent']].to_csv(output_path+'submission_SMOTE.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = list(fig.head(12).feature)\n",
    "features1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X[features1]\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_train1 = X_train[features1]\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "X_test1 = X_test[features1]\n",
    " \n",
    "pipe = Pipeline([\n",
    "        ('sc', StandardScaler()),\n",
    "        ('rf', RandomForestClassifier(max_depth=9, \n",
    "                                       min_samples_leaf=0.1,\n",
    "                                      min_samples_split=0.1,\n",
    "                                      n_estimators=100))\n",
    "         ])\n",
    "\n",
    "model = pipe.fit(X_train1, y_train)\n",
    "score = model.score(X_test1, y_test)\n",
    "\n",
    "# print results\n",
    "print(\"Model score:\", score)\n",
    "print(\"Cross validation scores mean:\", round(cross_val_score(model,X1,y,cv=3,scoring='roc_auc').mean(),5))\n",
    "print(\"Cross validation scores std dev:\", round(cross_val_score(model,X1,y,cv=3,scoring='roc_auc').std(),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv for kaggle submission\n",
    "test = test[features1]\n",
    "\n",
    "pred = pd.DataFrame(model.predict_proba(test), columns=['0','1']) \n",
    "submission = pd.DataFrame()\n",
    "submission['WnvPresent'] = pred['1']\n",
    "submission['Id'] = submission.index + 1\n",
    "submission[['Id', 'WnvPresent']].to_csv(output_path+'submission_SMOTE1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Evaluation & Next Steps\n",
    "\n",
    "The results from Kaggle is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename= image_path + 'submission_rfsmote1.PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename= image_path + 'submission_rfSMOTE.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are comparable with the method of oversampling on the minority class when all features were used. The kaggle scores decreased after selecting features through feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE is closely related to the real-life scenario where presence of virus is expected to cluster in vicinity of an infected area. \n",
    "\n",
    "\n",
    "Test score reflected much lower variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test1)\n",
    "cm = confusion_matrix(y_test, prediction)  #tn, fp, fn, tp\n",
    "pd.DataFrame(data=cm, columns=['predicted no wnv', 'predicted wmv'], index=['actual no wnv', 'actual wbv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
