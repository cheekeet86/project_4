{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Preprocessing & Modelling - Regression Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the prediction of number of mosquitos to be used as a feature in predicting the probability of virus.\n",
    "\n",
    "### Contents:\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Import Data](#Import-Data)\n",
    "- [Data prepared for Modelling](#Data-prepared-for-Modelling)\n",
    "- [Modelling](#Modelling)\n",
    "- [Model Evaluation](#Model-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "We import the necessary libraries used in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# maths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visual\n",
    "#from matplotlib_venn import venn2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# modelling\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# Others\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data\n",
    "\n",
    "The cleaned datasets are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "\n",
    "input_path = '../data/2_input/'\n",
    "clean_path = '../data/3_clean/'\n",
    "output_path = '../data/4_output/'\n",
    "\n",
    "image_path = '../images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import clean data\n",
    "\n",
    "df_train = pd.read_csv(clean_path + 'train_clean.csv')\n",
    "df_test = pd.read_csv(clean_path + 'test_clean.csv')\n",
    "df_weather = pd.read_csv(clean_path + 'weather_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of train dataset: {}'.format(df_train.shape))\n",
    "print('Size of test dataset: {}'.format(df_test.shape))\n",
    "print('Size of weather dataset: {}'.format(df_weather.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prepared for Modelling\n",
    "\n",
    "We combine train and test data to prepare the datasets for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops id column from test not in train\n",
    "test = df_test.drop('id', axis=1)\n",
    "#Drops nummosquitos and wnvpresent columns from train not in test\n",
    "train = df_train.drop(['nummosquitos', 'wnvpresent'], axis=1)\n",
    "\n",
    "#Combines train and test datasets\n",
    "combined_train_test = pd.concat([test,train])\n",
    "\n",
    "print('Size of train/test dataset: {}'.format(combined_train_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data from each station and use only the data from Station 1 here, considering that there were many null values in Station 2 which we imputed from Station 1 data when cleaning the data.\n",
    "We then, merge the Station 1 weather data with the train/test dataset to add information on weather conditions as measured at Station 1 on the dates of virus test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits weather data by Station\n",
    "only_station_1 = df_weather[df_weather['station'] == 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines weather data with train and test dataset\n",
    "all_dataset = combined_train_test.merge(only_station_1, how='left', on=['year','month','day'])\n",
    "\n",
    "print('Size of train/test dataset with weather data: {}'.format(all_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints train/test dataset with weather information\n",
    "all_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts categorical data into numeric\n",
    "df_get_dum = pd.concat([all_dataset, pd.get_dummies(all_dataset[['species', 'street', 'trap']],drop_first=True)], axis=1)\n",
    "df_get_dum.drop(['species', 'street', 'trap'], inplace =True, axis=1)\n",
    "\n",
    "print('Size of train/test dataset with weather data(One Hot Encoded): {}'.format(df_get_dum.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop 'block' as the information is used to derive the latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'block' column\n",
    "df_get_dum.drop(['block'], inplace =True, axis=1)\n",
    "print('Size of train/test dataset with weather data(after dropping block): {}'.format(df_get_dum.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data back into seperate train and test datasets and only use train for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits out train dataset using year\n",
    "train = df_get_dum[df_get_dum['year']%2!=0]\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#Re-attaching original nummosquitos and wnvpresent columns\n",
    "wnv = pd.Series(df_train['wnvpresent'])\n",
    "train_with_wnv = pd.concat([train , wnv], axis=1)\n",
    "train_with_wnv['nummosquitos'] = df_train['nummosquitos']\n",
    "\n",
    "print('Size of processed train data: {}'.format(train_with_wnv.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits out test dataset using year\n",
    "test = df_get_dum.loc[df_get_dum['year']%2==0]\n",
    "print('Size of processed test data: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also note that the data is imbalanced. Out of the 8475 rows in our training dataset, only 457 (~5%) data points represent the virus present class while 8018 represent virus not present. \n",
    "\n",
    "The method to handle the imbalanced data is the same as the main modelling notebook (3. Preprocessing & Modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits data by presence of wnv\n",
    "majority_class = train_with_wnv[train_with_wnv['wnvpresent']==0]\n",
    "minority_class = train_with_wnv[train_with_wnv['wnvpresent']==1]\n",
    "\n",
    "#Resamples minority class with replacement\n",
    "minority_upsampled = resample( minority_class, replace=True, n_samples=majority_class.shape[0], random_state=42)\n",
    "\n",
    "#Combine new minority class dataset with original majority class dataset\n",
    "train_resampled = pd.concat([minority_upsampled,majority_class])\n",
    "\n",
    "#Checks class representation\n",
    "train_resampled.wnvpresent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffles dataset to inject randomness\n",
    "df = shuffle(train_resampled, random_state=42)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print resampled, reshuffled new dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "#### Predict number of mosquitos\n",
    "\n",
    "We use all the features in the dataset to fit regression models and identify nummosquitos to be our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['nummosquitos', 'wnvpresent'])\n",
    "y = df.nummosquitos\n",
    "\n",
    "# data is split randomly into train and test subsets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model on the following regressions and test its performance using R2 score: \n",
    "- Linear Regression, \n",
    "- Ridge, \n",
    "- Lasso, \n",
    "- Elastic Net,\n",
    "- Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {\n",
    "    'linreg': LinearRegression(),\n",
    "    'ridge': Ridge(),\n",
    "    'lasso': Lasso(),\n",
    "    'en': ElasticNet(),\n",
    "    'rfr': RandomForestRegressor()\n",
    "}.items()\n",
    "\n",
    "for k,v in estimators:\n",
    "    pipe = Pipeline([\n",
    "        ('sc', StandardScaler()),\n",
    "        (k,v)])\n",
    "    model = pipe.fit(X_train,y_train)\n",
    "    print('{} score: {}, Cross Validation Mean: {}, Cross Validation Std Dev: {}'.format(k, model.score(X_train,y_train), round(cross_val_score(model,X,y,cv=3).mean(),5), round(cross_val_score(model,X,y,cv=3).std(),5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default parameters, the R2 scores for all models are very low except for random forest regressor. As random forest regressor is a bagging technique, the aggregation of regression of random features will produce the highest score. As such, we chose to focus on random forest regressor to tune the hyperparameters and take a look at the features that are more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Regressor has the highest score in predicting the number of mosquitos. \n",
    "pipe = Pipeline([\n",
    "        ('sc', StandardScaler()),\n",
    "        ('rfr', RandomForestRegressor(n_estimators=20))\n",
    "         ])\n",
    "\n",
    "params = {'rfr__n_estimators': [10, 30, 50, 100]}\n",
    "\n",
    "gridsearch = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=params,\n",
    "        verbose=1,\n",
    "        n_jobs=3,\n",
    "    )\n",
    "\n",
    "gridsearch.fit(X_train, y_train)\n",
    "    \n",
    "model = gridsearch.best_estimator_\n",
    "score = model.score(X_test, y_test)\n",
    "best_params = gridsearch.best_params_\n",
    "\n",
    "# print results\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best R2 score:\", gridsearch.best_score_)\n",
    "print(\"Test R2 score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = model.named_steps['rfr']\n",
    "\n",
    "fi = pd.DataFrame({\n",
    "    'features': X.columns,\n",
    "    'importances': regressor.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = fi.sort_values(by='importances', ascending=False).iloc[:40]\n",
    "fig.plot(kind='barh', figsize=(10,9))\n",
    "plt.yticks(range(len(fig)),fig['features'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, the feature importance becomes very small after the 31st feature. Hence, we will take the top 31 features for Random forest regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features1 = list(fig.head(31).features)\n",
    "features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X1 = df[features1]\n",
    "X_train1 = X_train[features1]\n",
    "X_test1 = X_test[features1]\n",
    " \n",
    "pipe = Pipeline([\n",
    "        ('sc', StandardScaler()),\n",
    "        ('rfr', RandomForestRegressor(n_estimators=30))\n",
    "         ])\n",
    "\n",
    "model = pipe.fit(X_train1, y_train)\n",
    "score = model.score(X_test1, y_test)\n",
    "\n",
    "# print results\n",
    "print(\"R2 score:\", score)\n",
    "print(\"Cross validation scores mean:\", round(cross_val_score(model,X1,y,cv=3).mean(),5))\n",
    "print(\"Cross validation scores std dev:\", round(cross_val_score(model,X1,y,cv=3).std(),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the cross validation scores is close to the R2 score and the standard deviation is low, indicating that the model can be generalised to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction using kaggle test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the features selected for kaggle test set\n",
    "X_kaggle = test[features1]\n",
    "X_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predicting number of mosquitos from kaggle test set\n",
    "test['nummosquitos'] = model.predict(X_kaggle)\n",
    "test['nummosquitos'] = test['nummosquitos'].map(lambda x:int(x))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict presence of west nile virus\n",
    "\n",
    "We use all the features in the dataset to for classification and identify wnvpresent to be our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['wnvpresent'])\n",
    "y = df.wnvpresent\n",
    "\n",
    "# data is split randomly into train and test subsets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a model using Random Forest Classifer and tune the hyperparameters with GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators' : [10, 50, 100],\n",
    "    'max_depth' : [3,9,15,20],\n",
    "    'min_samples_split': np.linspace(0.1, 0.5, 5),\n",
    "    'min_samples_leaf' : np.linspace(0.1, 0.5, 5),\n",
    "    'max_features' : (20, 50, 200, None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = []\n",
    "roc_auc = []\n",
    "\n",
    "gridsearch = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=params,\n",
    "    verbose=1,\n",
    "    cv= 3,\n",
    "    n_jobs=-1,\n",
    "    return_train_score= True,\n",
    "    scoring = 'roc_auc'\n",
    ")\n",
    "\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "model = gridsearch.best_estimator_\n",
    "cv_score = gridsearch.cv_results_\n",
    "best_params = gridsearch.best_params_\n",
    "\n",
    "# predict y\n",
    "y_pred = pd.DataFrame(model.predict_proba(X_test), columns=['0','1'])\n",
    "\n",
    "# print results\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", gridsearch.best_score_)\n",
    "print(\"AUC/ROC test:\", roc_auc_score(y_test,y_pred['1']))\n",
    "pd.set_option('display.max_rows', 750)\n",
    "display(pd.DataFrame(cv_score, columns = cv_score.keys()), )\n",
    "\n",
    "\n",
    "# append info to list\n",
    "parameters.append(best_params)\n",
    "roc_auc.append(roc_auc_score(y_test,y_pred['1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Deviation of train score is low, indicating that the model is not overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise feature importances from the model\n",
    "fi = pd.DataFrame({\n",
    "    'features': X.columns,\n",
    "    'importances': model.feature_importances_})\n",
    "fig = fi.sort_values(by='importances', ascending=False).iloc[:30]\n",
    "fig.plot(kind='barh', figsize=(10,9))\n",
    "plt.yticks(range(len(fig)),fig['features'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv for kaggle submission\n",
    "features = list(X.columns)\n",
    "test = test[features]\n",
    "\n",
    "pred = pd.DataFrame(model.predict_proba(test), columns=['0','1']) \n",
    "submission = pd.DataFrame()\n",
    "submission['WnvPresent'] = pred['1']\n",
    "submission['Id'] = submission.index + 1\n",
    "submission[['Id', 'WnvPresent']].to_csv(output_path+'submission_with_regression.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot on feature importances above, the feature importance becomes very small (i.e. insignificant) after the 22nd feature. Hence, we will take the top 22 features for Random forest classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features1 = list(fig.head(22).features)\n",
    "\n",
    "X1 = X[features1]\n",
    "X_train1 = X_train[features1]\n",
    "X_test1 = X_test[features1]\n",
    " \n",
    "pipe = Pipeline([\n",
    "        ('sc', StandardScaler()),\n",
    "        ('rf', RandomForestClassifier(max_depth=9, \n",
    "                                       #max_features=50, \n",
    "                                       min_samples_leaf=0.1,\n",
    "                                      min_samples_split=0.2,\n",
    "                                      n_estimators=100))\n",
    "         ])\n",
    "\n",
    "model = pipe.fit(X_train1, y_train)\n",
    "score = model.score(X_test1, y_test)\n",
    "\n",
    "# print results\n",
    "print(\"Model score:\", score)\n",
    "print(\"Cross validation scores mean:\", round(cross_val_score(model,X1,y,cv=3,scoring='roc_auc').mean(),5))\n",
    "print(\"Cross validation scores std dev:\", round(cross_val_score(model,X1,y,cv=3,scoring='roc_auc').std(),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation of cross validation scores is low, indicating that the model should be able to generalise to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv for kaggle submission\n",
    "\n",
    "test = test[features1]\n",
    "\n",
    "pred = pd.DataFrame(model.predict_proba(test), columns=['0','1']) \n",
    "submission = pd.DataFrame()\n",
    "submission['WnvPresent'] = pred['1']\n",
    "submission['Id'] = submission.index + 1\n",
    "submission[['Id', 'WnvPresent']].to_csv(output_path+'submission_with_regression1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from Kaggle is appended below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename= image_path + 'submission_with_regression.PNG') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resubmitted to kaggle after reducing the features to the top 22 from feature importance\n",
    "Image(filename= image_path + 'submission_with_regression1.PNG') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the private and public kaggle score is small but the difference between the kaggle score and model score is large, indicating that our model is not very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test1)\n",
    "cm = confusion_matrix(y_test, prediction)  #tn, fp, fn, tp\n",
    "\n",
    "pd.DataFrame(data=cm, columns=['predicted no wnv', 'predicted wmv'], index=['actual no wnv', 'actual wbv'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
