{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Test and Train Dataset\n",
    "\n",
    "In this notebook, we will clean the test and train dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# maths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime as datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "\n",
    "input_path = '../data/2_input/'\n",
    "clean_path = '../data/3_clean/'\n",
    "output_path = '../data/4_output/'\n",
    "\n",
    "image_path = '../images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Inspect Data\n",
    "\n",
    "The train dataset has 10,506 entries and 12 columns while the test dataset has 116,293 entries and 11 columns. The `NumMosquitos` and `WnvPresent` are missing from the test dataset and these are our predictor values (i.e. y). The test dataset also has a `ID` column, to identify every entry. \n",
    "\n",
    "Several of these columns such as `Address`, `Block` and `AddressNumberAndStreet` pertain to location details. We decided to retain only the `latitude` and `longitude` columns and drop all other location related features for both train and test datasets as they are deemed redundant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(input_path+'train.csv')\n",
    "test = pd.read_csv(input_path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data\n",
    "\n",
    "We created functions to rename the columns and drop the redundant location related features. We also decided to split the date column into `year`, `month` and `day` columns so that it will be easier for us to analyse the seasonality effect later on. \n",
    "\n",
    "As explained in the Kaggle competition, the train and test datasets are organised in such a way that when the number of mosquitos exceed 50, they are split into another record (another row in the dataset), such that the number of mosquitos are capped at 50. We have thus also ran a for-loop to combine these duplicate rows and sum up the `NumMosquitos`.\n",
    "\n",
    "We then combined both train and test datasets and saved them as csv files for exploratory data analysis later on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yr(x): \n",
    "    return x.split('-')[0] \n",
    "\n",
    "def create_mth(x): \n",
    "    return x.split('-')[1] \n",
    "\n",
    "def create_day(x): \n",
    "    return x.split('-')[2] \n",
    "\n",
    "def rename_columns (columns):\n",
    "    return [column.lower() for column in columns]\n",
    "\n",
    "def clean_data(df): \n",
    "    df['year'] = df.Date.apply(create_yr)\n",
    "    df['month'] = df.Date.apply(create_mth)\n",
    "    df['day'] = df.Date.apply(create_day)    \n",
    "\n",
    "    df.drop(['Address', 'AddressNumberAndStreet', 'AddressAccuracy', 'Date'], axis = 1, inplace = True)\n",
    "    df.columns = rename_columns(df.columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = clean_data(train)\n",
    "test = clean_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge nummosquitos and wnvpresent for duplicated rows\n",
    "\n",
    "# create train_2 (new copy)\n",
    "# to compare train and train_2 and verify code\n",
    "train_2 = train.copy()\n",
    "\n",
    "# duplicated rows have the same values for the columns below\n",
    "cols = ['species','trap','year','month', 'day','latitude','longitude']\n",
    "\n",
    "for row_idx,row in train_2.iterrows():\n",
    "    \n",
    "    # skip 1st row\n",
    "    if row_idx > 0:\n",
    "        \n",
    "        # reset counter for each row\n",
    "        duplicate_count = 0        \n",
    "        \n",
    "        for col in cols:\n",
    "            \n",
    "            # compare cells in current and previous rows\n",
    "            # increment counter if both cells have the same value \n",
    "            if train_2.at[row_idx,col] == train_2.at[row_idx-1,col]:\n",
    "                duplicate_count += 1               \n",
    "                \n",
    "        # counter equal to number of selected column\n",
    "        # current row['nummosquitos','wnvpresent'] = sum of current and previous rows\n",
    "        if duplicate_count == len(cols):\n",
    "            train_2.at[row_idx,'nummosquitos'] = train_2.at[row_idx,'nummosquitos'] + train_2.at[row_idx-1,'nummosquitos']\n",
    "            train_2.at[row_idx,'wnvpresent'] = train_2.at[row_idx,'wnvpresent'] + train_2.at[row_idx-1,'wnvpresent']\n",
    "\n",
    "# remove duplicated rows (keep only last row)\n",
    "train_2.drop_duplicates(subset=cols, keep='last',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check total nummosquitos and wnvpresent\n",
    "\n",
    "print(train['nummosquitos'].sum())\n",
    "print(train['wnvpresent'].sum())\n",
    "print('')\n",
    "print(train_2['nummosquitos'].sum())\n",
    "print(train_2['wnvpresent'].sum())\n",
    "print('')\n",
    "print(train_2['wnvpresent'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wnvpresent: total count to 0 or 1\n",
    "\n",
    "for row_idx,row in train_2.iterrows():\n",
    "    \n",
    "    if row['wnvpresent'] > 1:\n",
    "        #print(row_idx,row['wnvpresent'])\n",
    "        train_2.at[row_idx,'wnvpresent'] = 1\n",
    "        \n",
    "print(train_2['wnvpresent'].sum())\n",
    "print(train_2['wnvpresent'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine df_train and df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataframe info\n",
    "\n",
    "print(train_2.shape)\n",
    "print(test.shape)\n",
    "print('')\n",
    "print(train_2.columns)\n",
    "print('')\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deep copy\n",
    "train_3 = train_2.copy()\n",
    "test_3 = test.copy() # copy to test_3 instead of test_2\n",
    "\n",
    "# train: missing id column\n",
    "# add id column\n",
    "train_3['is_train'] = 1\n",
    "train_3['id'] = -1\n",
    "\n",
    "# test: missing nummosquitos and wnvpresent columns\n",
    "# add this columns\n",
    "test_3['is_train'] = 0\n",
    "test_3['nummosquitos'] = -1\n",
    "test_3['wnvpresent'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train\n",
    "\n",
    "train_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test\n",
    "\n",
    "test_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and test\n",
    "\n",
    "train_test = pd.concat([train_3,test_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train_test\n",
    "\n",
    "train_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2.to_csv(clean_path+'train_clean.csv',index=False)\n",
    "test.to_csv(clean_path+'test_clean.csv',index=False)\n",
    "\n",
    "train_test.to_csv(clean_path+'train_test_clean.csv',index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
